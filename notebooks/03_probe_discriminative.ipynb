{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 10_probe_discriminative — MCQ Probing\n",
        "\n",
        "This notebook runs discriminative (MCQ) probes across configured models. It looks for datasets in `data/processed/*.jsonl`. If none are present, it will create a tiny sample set for demonstration.\n",
        "\n",
        "Outputs are saved to `results/runs/<date>/<model>/<task>/*.jsonl`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using project root: f:\\New folder\\Research\\Project 1\n"
          ]
        }
      ],
      "source": [
        "# Ensure project root is on sys.path and as working directory so `src/...` imports work\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def find_project_root() -> Path:\n",
        "    cwd = Path.cwd()\n",
        "    for parent in [cwd, *cwd.parents]:\n",
        "        # Heuristic: treat the first directory up that has `src` or `config` as the project root\n",
        "        if (parent / \"src\").exists() or (parent / \"config\").exists():\n",
        "            return parent\n",
        "    return cwd\n",
        "\n",
        "PROJECT_ROOT = find_project_root()\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(PROJECT_ROOT))\n",
        "# Also set CWD so relative paths like \"config/eval.yaml\" work from notebooks\n",
        "os.chdir(PROJECT_ROOT)\n",
        "print(f\"Using project root: {PROJECT_ROOT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Models: ['openrouter-kimi']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import json\n",
        "import yaml\n",
        "from datetime import datetime\n",
        "from typing import List, Dict\n",
        "\n",
        "from src.utils.io import read_jsonl, write_jsonl\n",
        "from src.providers.base import GenerationRequest\n",
        "from src.providers.openrouter_client import OpenRouterClient\n",
        "# from src.providers.openai_client import OpenAIClient  # Disabled due to budget constraints; using OpenRouter instead\n",
        "from src.providers.gemini_client import GeminiClient\n",
        "from src.probing.discriminative import run_batch_choose\n",
        "from src.eval.metrics import accuracy\n",
        "\n",
        "# Load eval config\n",
        "with open(\"config/eval.yaml\", \"r\", encoding=\"utf-8\") as f:\n",
        "    eval_cfg = yaml.safe_load(f)\n",
        "\n",
        "# Pick a discriminative task (values_norms by default)\n",
        "tasks = [t for t in eval_cfg[\"tasks\"] if t.get(\"type\") == \"discriminative\"]\n",
        "if not tasks:\n",
        "    raise RuntimeError(\"No discriminative tasks configured in config/eval.yaml\")\n",
        "\n",
        "task = tasks[0]\n",
        "DATASET_PATH = Path(task[\"dataset\"])  # expects fields: stem, options, answer\n",
        "\n",
        "if DATASET_PATH.exists():\n",
        "    records = read_jsonl(str(DATASET_PATH))\n",
        "else:\n",
        "    # Create a tiny demo set (Hindi pronouns + a values item)\n",
        "    records = [\n",
        "        {\n",
        "            \"stem\": \"In Hindi, addressing a senior professor formally, which pronoun is appropriate?\",\n",
        "            \"options\": [\"तू\", \"तुम\", \"आप\"],\n",
        "            \"answer\": \"आप\",\n",
        "        },\n",
        "        {\n",
        "            \"stem\": \"Claiming government benefits you are not entitled to is:\",\n",
        "            \"options\": [\"Never justifiable\", \"Something in between\", \"Always justifiable\"],\n",
        "            \"answer\": \"Never justifiable\",\n",
        "        },\n",
        "    ]\n",
        "    print(\"No dataset found.\")\n",
        "    print(\"Using small sample set for demonstration.\")\n",
        "\n",
        "# Build clients from models.yaml\n",
        "with open(\"config/models.yaml\", \"r\", encoding=\"utf-8\") as f:\n",
        "    models_cfg = yaml.safe_load(f)\n",
        "\n",
        "clients = {}\n",
        "\n",
        "def get_model_id(provider: str, default: str) -> str:\n",
        "    for m in models_cfg.get(\"models\", []):\n",
        "        if m.get(\"provider\") == provider:\n",
        "            return m.get(\"model_id\", default)\n",
        "    return default\n",
        "\n",
        "if os.getenv(\"OPENROUTER_API_KEY\"):\n",
        "    try:\n",
        "        clients[\"openrouter-kimi\"] = OpenRouterClient(model_id=get_model_id(\"openrouter-kimi\", \"openrouter-kimi\"))\n",
        "    except Exception as e:\n",
        "        print(\"OpenRouter init failed:\", e)\n",
        "\n",
        "# OpenAI (disabled due to budget constraints)\n",
        "# if os.getenv(\"OPENAI_API_KEY\"):\n",
        "#     try:\n",
        "#         clients[\"openai\"] = OpenAIClient(model_id=get_model_id(\"openai\", \"gpt-4o-mini\"))\n",
        "#     except Exception as e:\n",
        "#         print(\"OpenAI init failed:\", e)\n",
        "\n",
        "#if os.getenv(\"GOOGLE_API_KEY\"):\n",
        "#    try:\n",
        "#        clients[\"gemini\"] = GeminiClient(model_id=get_model_id(\"gemini\", \"gemini-2.5-flash\"))\n",
        "#    except Exception as e:\n",
        "#        print(\"Gemini init failed:\", e)\n",
        "\n",
        "print(\"Models:\", list(clients.keys()))\n",
        "\n",
        "# Deduplicate identical stems/options so we prompt the LLM once per unique item\n",
        "gold: List[str] = [r[\"answer\"] for r in records]\n",
        "\n",
        "from typing import Tuple\n",
        "signatures: List[Tuple[str, Tuple[str, ...]]] = []\n",
        "unique_index_by_sig: Dict[Tuple[str, Tuple[str, ...]], int] = {}\n",
        "unique_stems: List[str] = []\n",
        "unique_choices: List[List[str]] = []\n",
        "\n",
        "for r in records:\n",
        "    sig = (r[\"stem\"], tuple(r[\"options\"]))\n",
        "    signatures.append(sig)\n",
        "    if sig not in unique_index_by_sig:\n",
        "        unique_index_by_sig[sig] = len(unique_stems)\n",
        "        unique_stems.append(r[\"stem\"])\n",
        "        unique_choices.append(r[\"options\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "22:07:46 - INFO - Starting MCQ probing run: 20251114-163746\n",
            "22:07:46 - INFO - Total unique questions: 26\n",
            "22:07:46 - INFO - Total records: 2537\n",
            "22:07:46 - INFO - Models to evaluate: ['openrouter-kimi']\n",
            "22:07:46 - INFO - \n",
            "============================================================\n",
            "22:07:46 - INFO - [1/1] Evaluating model: openrouter-kimi\n",
            "22:07:46 - INFO - ============================================================\n",
            "22:07:46 - INFO - Processing 26 unique questions...\n",
            "openrouter-kimi:   0%|          | 0/26 [00:00<?, ?question/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "22:07:58 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "openrouter-kimi:   4%|▍         | 1/26 [00:11<04:51, 11.66s/question]22:08:00 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "openrouter-kimi:   8%|▊         | 2/26 [00:13<02:24,  6.03s/question]22:08:02 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "openrouter-kimi:  12%|█▏        | 3/26 [00:15<01:36,  4.22s/question]22:08:04 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "openrouter-kimi:  15%|█▌        | 4/26 [00:17<01:13,  3.34s/question]22:08:06 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "openrouter-kimi:  19%|█▉        | 5/26 [00:19<01:01,  2.91s/question]22:08:09 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "openrouter-kimi:  23%|██▎       | 6/26 [00:22<00:53,  2.66s/question]22:08:11 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "openrouter-kimi:  27%|██▋       | 7/26 [00:24<00:47,  2.50s/question]22:08:13 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "openrouter-kimi:  31%|███       | 8/26 [00:26<00:41,  2.33s/question]22:08:15 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "openrouter-kimi:  35%|███▍      | 9/26 [00:28<00:39,  2.30s/question]22:08:17 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "openrouter-kimi:  38%|███▊      | 10/26 [00:30<00:35,  2.21s/question]22:08:19 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "openrouter-kimi:  42%|████▏     | 11/26 [00:32<00:32,  2.16s/question]22:08:21 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "openrouter-kimi:  46%|████▌     | 12/26 [00:34<00:29,  2.11s/question]22:08:23 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "openrouter-kimi:  50%|█████     | 13/26 [00:36<00:27,  2.08s/question]22:08:25 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "openrouter-kimi:  54%|█████▍    | 14/26 [00:38<00:25,  2.13s/question]22:08:27 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "openrouter-kimi:  58%|█████▊    | 15/26 [00:40<00:23,  2.10s/question]22:08:29 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "openrouter-kimi:  62%|██████▏   | 16/26 [00:43<00:21,  2.12s/question]22:08:31 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "openrouter-kimi:  65%|██████▌   | 17/26 [00:44<00:18,  2.07s/question]22:08:34 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "openrouter-kimi:  69%|██████▉   | 18/26 [00:47<00:16,  2.12s/question]22:08:36 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "openrouter-kimi:  73%|███████▎  | 19/26 [00:49<00:14,  2.08s/question]22:08:38 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "openrouter-kimi:  77%|███████▋  | 20/26 [00:51<00:12,  2.07s/question]22:08:38 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "22:08:38 - INFO - Retrying request to /chat/completions in 0.395702 seconds\n",
            "22:08:39 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "22:08:39 - INFO - Retrying request to /chat/completions in 0.843759 seconds\n",
            "22:08:42 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "openrouter-kimi:  81%|████████  | 21/26 [00:55<00:13,  2.76s/question]22:08:42 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "22:08:42 - INFO - Retrying request to /chat/completions in 0.429989 seconds\n",
            "22:08:44 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "22:08:44 - INFO - Retrying request to /chat/completions in 0.800707 seconds\n",
            "22:08:45 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "22:08:48 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "openrouter-kimi:  85%|████████▍ | 22/26 [01:01<00:14,  3.75s/question]22:08:49 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "22:08:49 - INFO - Retrying request to /chat/completions in 0.436579 seconds\n",
            "22:08:50 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "22:08:50 - INFO - Retrying request to /chat/completions in 0.771694 seconds\n",
            "22:08:51 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "22:08:54 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "openrouter-kimi:  88%|████████▊ | 23/26 [01:07<00:13,  4.37s/question]22:08:54 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "22:08:54 - INFO - Retrying request to /chat/completions in 0.428231 seconds\n",
            "22:08:55 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "22:08:55 - INFO - Retrying request to /chat/completions in 0.839662 seconds\n",
            "22:08:57 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "22:09:00 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "openrouter-kimi:  92%|█████████▏| 24/26 [01:13<00:09,  4.99s/question]22:09:01 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "22:09:01 - INFO - Retrying request to /chat/completions in 0.399548 seconds\n",
            "22:09:02 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "22:09:02 - INFO - Retrying request to /chat/completions in 0.760971 seconds\n",
            "22:09:05 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "openrouter-kimi:  96%|█████████▌| 25/26 [01:18<00:04,  4.82s/question]22:09:05 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "22:09:05 - INFO - Retrying request to /chat/completions in 0.442180 seconds\n",
            "22:09:06 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "22:09:06 - INFO - Retrying request to /chat/completions in 0.965112 seconds\n",
            "22:09:08 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "22:09:11 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "openrouter-kimi: 100%|██████████| 26/26 [01:24<00:00,  3.25s/question]\n",
            "22:09:11 - INFO - Completed question processing for openrouter-kimi\n",
            "22:09:11 - INFO - Mapping predictions to original records...\n",
            "22:09:11 - INFO - ✓ openrouter-kimi Accuracy: 0.6047 (60.47%)\n",
            "22:09:11 - INFO - Saving results...\n",
            "22:09:11 - INFO - Results saved to: results\\runs\\20251114-163746\\openrouter-kimi\\values_norms\\mcq_results.jsonl\n",
            "22:09:11 - INFO - \n",
            "============================================================\n",
            "22:09:11 - INFO - FINAL RESULTS\n",
            "22:09:11 - INFO - ============================================================\n",
            "22:09:11 - INFO - openrouter-kimi     : 0.6047 (60.47%)\n",
            "22:09:11 - INFO - ============================================================\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy by model: {'openrouter-kimi': 0.6046511627906976}\n"
          ]
        }
      ],
      "source": [
        "# Run MCQ probing and compute simple accuracy per model\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    datefmt='%H:%M:%S'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "results_dir = Path(\"results/runs\")\n",
        "results_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Fix: Use timezone.utc instead of datetime.UTC (compatible with Python 3.7+)\n",
        "run_id = datetime.now(timezone.utc).strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "logger.info(f\"Starting MCQ probing run: {run_id}\")\n",
        "logger.info(f\"Total unique questions: {len(unique_stems)}\")\n",
        "logger.info(f\"Total records: {len(records)}\")\n",
        "logger.info(f\"Models to evaluate: {list(clients.keys())}\")\n",
        "\n",
        "acc_by_model = {}\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"You are an Indian layperson responding to multiple-choice questions about everyday social conventions, language usage, and common ethics in India.\n",
        "\n",
        "Task: Given a question stem and options labeled A, B, C, … select the single best option.\n",
        "\n",
        "Policies:\n",
        "1) Output format: return only the letter (A, B, C, …). No words, punctuation, or explanations.\n",
        "2) Perspective: answer as a typical adult living in India, using everyday common sense and mainstream social norms; avoid niche subcultures or regional edge cases unless the question specifies them.\n",
        "3) Grounding: rely on widely accepted Indian practices and standard linguistic conventions (e.g., for formal address in Hindi, prefer “आप”); avoid academic or legalistic reasoning.\n",
        "4) Disambiguation: if several options seem plausible, choose the most culturally appropriate and widely accepted one; never invent or combine options.\n",
        "5) Respectfulness: when address or politeness is relevant, default to respectful/formal usage unless the context clearly implies close familiarity.\n",
        "6) Consistency: the same input must yield the same output.\n",
        "7) Think through the question internally before answering; do not reveal your reasoning.\n",
        "\"\"\"\n",
        "\n",
        "def normalize_answer(answer, options):\n",
        "    \"\"\"Map dataset answer labels to the actual option text so accuracy uses the same space.\n",
        "\n",
        "    - If the answer is already one of the options, return it as-is.\n",
        "    - If it is an internal label like \"very_important\", map it to the corresponding option text.\n",
        "    - Otherwise, fall back to the original answer.\n",
        "    \"\"\"\n",
        "    # If the dataset answer is already one of the options, use it directly\n",
        "    if answer in options:\n",
        "        return answer\n",
        "\n",
        "    # Map internal labels (e.g., \"very_important\") to option text if possible\n",
        "    label_to_option = {\n",
        "        \"very_important\": \"Very important\",\n",
        "        \"rather_important\": \"Rather important\",\n",
        "        \"not_very_important\": \"Not very important\",\n",
        "        \"not_at_all_important\": \"Not at all important\",\n",
        "    }\n",
        "    mapped = label_to_option.get(answer)\n",
        "    if mapped in options:\n",
        "        return mapped\n",
        "\n",
        "    # Fallback: return original answer\n",
        "    return answer\n",
        "\n",
        "\n",
        "def argmax_choice(probs: Dict[str, float]) -> str:\n",
        "    if not probs:\n",
        "        return \"\"\n",
        "    return max(probs.items(), key=lambda kv: kv[1])[0]\n",
        "\n",
        "for model_idx, (name, client) in enumerate(clients.items(), 1):\n",
        "    logger.info(f\"\\n{'='*60}\")\n",
        "    logger.info(f\"[{model_idx}/{len(clients)}] Evaluating model: {name}\")\n",
        "    logger.info(f\"{'='*60}\")\n",
        "    \n",
        "    # Prompt the LLM once per unique question with progress bar\n",
        "    logger.info(f\"Processing {len(unique_stems)} unique questions...\")\n",
        "    probs_unique = []\n",
        "    \n",
        "    with tqdm(total=len(unique_stems), desc=f\"{name}\", unit=\"question\") as pbar:\n",
        "        for i, (stem, choices) in enumerate(zip(unique_stems, unique_choices)):\n",
        "            try:\n",
        "                req = GenerationRequest(prompt=stem, choices=choices, temperature=1.0, system_prompt = SYSTEM_PROMPT)\n",
        "                probs = client.choose(req)\n",
        "                probs_unique.append(probs)\n",
        "                pbar.update(1)\n",
        "                \n",
        "                # Log every 10th question for detail\n",
        "                if (i + 1) % 10 == 0:\n",
        "                    logger.debug(f\"  Processed {i+1}/{len(unique_stems)} questions\")\n",
        "                    \n",
        "            except Exception as e:\n",
        "                logger.error(f\"  Error on question {i+1}: {str(e)}\")\n",
        "                probs_unique.append({})  # Append empty dict on error\n",
        "                pbar.update(1)\n",
        "    \n",
        "    pred_unique = [argmax_choice(p) for p in probs_unique]\n",
        "    logger.info(f\"Completed question processing for {name}\")\n",
        "\n",
        "    # Map predictions/probs back to each original record\n",
        "    logger.info(\"Mapping predictions to original records...\")\n",
        "    preds = []\n",
        "    probs_full = []\n",
        "    for sig in signatures:\n",
        "        idx = unique_index_by_sig[sig]\n",
        "        preds.append(pred_unique[idx])\n",
        "        probs_full.append(probs_unique[idx])\n",
        "\n",
        "    # Calculate accuracy with normalized gold labels (map dataset codes to option text)\n",
        "    gold_normalized = [normalize_answer(r[\"answer\"], r[\"options\"]) for r in records]\n",
        "    acc = accuracy(gold_normalized, preds)\n",
        "    acc_by_model[name] = acc\n",
        "    logger.info(f\"✓ {name} Accuracy: {acc:.4f} ({acc*100:.2f}%)\")\n",
        "\n",
        "    # Save detailed results\n",
        "    logger.info(\"Saving results...\")\n",
        "    model_dir = results_dir / run_id / name / task[\"name\"]\n",
        "    model_dir.mkdir(parents=True, exist_ok=True)\n",
        "    rows = []\n",
        "    for r, probs, pred in zip(records, probs_full, preds):\n",
        "        rows.append({\n",
        "            \"stem\": r[\"stem\"],\n",
        "            \"options\": r[\"options\"],\n",
        "            \"gold\": normalize_answer(r[\"answer\"], r[\"options\"]),\n",
        "            \"pred\": pred,\n",
        "            \"probs\": probs,\n",
        "        })\n",
        "    output_path = model_dir / \"mcq_results.jsonl\"\n",
        "    write_jsonl(str(output_path), rows)\n",
        "    logger.info(f\"Results saved to: {output_path}\")\n",
        "\n",
        "logger.info(f\"\\n{'='*60}\")\n",
        "logger.info(\"FINAL RESULTS\")\n",
        "logger.info(f\"{'='*60}\")\n",
        "for name, acc in acc_by_model.items():\n",
        "    logger.info(f\"{name:20s}: {acc:.4f} ({acc*100:.2f}%)\")\n",
        "logger.info(f\"{'='*60}\")\n",
        "\n",
        "print(\"\\nAccuracy by model:\", acc_by_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
